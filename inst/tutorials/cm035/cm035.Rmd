---
title: "Tutorial"
output: 
  learnr::tutorial:
    theme: flatly
runtime: shiny_prerendered
---

```{r setup, include=FALSE}
library(learnr)
library(tidyverse)
knitr::opts_chunk$set(echo = FALSE)
```



## Probability Distributions in R

### Exercise 

*Here's a simple exercise with an empty code chunk provided for entering the answer.*

```{r eval = FALSE}
help(Distributions)
```


Generally, there are two types of distributions: continuous and discrete.

| Type | Distributions | In `R` | Parameters | Default Values |
| :--- | :------------:| :----: | :--------: | :------------: |
| Continuous | Beta ($\beta$)         | `beta` | shape1, shape2 | |
|            | Cauchy                 | `cauchy` | location, scale | 0, 1 |
|            | Chi-square ($\chi^2$)  | `chisq` | degrees of freedom (df)| |
|            | Exponential            | `exp` | 1/mean | 1 |
|            | *F*                    | `f` | df1, df2 | |
|            | Gamma ($\gamma$)       | `gamma`| shape, 1/scale | NA, 1 |
|            | Normal                 | `norm`| mean, standard deviation (sd) | 0, 1
|            | Student's *t*          | `t` | df | |
|            | Uniform                | `unif`| min, max | 0, 1
|            | Weibull                | `weibull` | shape | |
| Discrete   | Binomial               | `binom` | size, probability | |
|            | Geometric              | `geom` | probability | |
|            | Hypergeometric         | `hyper` | m, n, k | |
|            | Poisson                | `pois` | lambda | |


For every distribution there are four prefixes. The parameters (in parentheses) for each depend on the *core distribution*:

| `R` command | Returns |
| ------- |:-------:|
| `d___()` | the height of the probability density function (probability mass function for discrete) |
| `p___()` | the cumulative density function |
| `q___()` | the inverse cumulative density function (quantiles) |
| `r___()` | randomly generated numbers |



## Finding p-values

From [Wikipedia](https://en.wikipedia.org/wiki/P-value):

> "The p-value is defined as the probability, under the assumption of the null hypothesis, of obtaining a result equal to or more extreme than what was actually observed.""

- Depending on how it is looked at, the "more extreme than what was actually observed" can mean
    -  $X \geq x$ (right-tailevent) or
    -  $X \leq x$ (left-tailevent) or
    - the "smaller" of $X \leq x$ and $X \geq x$ (double-tailed event).
- Thus, the p-value is given by:
    - $Pr(X \geq x | H)$ for right tail event,
    - $Pr(X \leq x | H)$ for left tail event,
    - ${\displaystyle 2 \times \min\{\Pr(X\leq x|H),\Pr(X\geq x|H)\}}$ for double tail event (also read here: https://math.stackexchange.com/questions/1493880/two-tailed-hypothesis-test-why-do-we-multiply-p-value-by-two)



To find a p-value, you first need to know if the null distribution you are working with is discrete or continuous.

- If discrete, you'll want to use `d__()`. This provides a *point* probability- you'll need to sum to find the probability of a range.
- If continuous, you'll want to use `p__()`. The probability for a single point within a continuous distribution is 0- you can only calculate probability across a range of continuous values. This function takes a quantile (or a vector of quantiles) for the named distribution as input, and returns a probability (between 0 and 1).

### 1.

IQ scores have a distribution that is approximately normal in shape, with a mean of 100 and a standard deviation of 15. What percentage of scores is at or above an IQ of 116? (put a `#` before the line of code you don't edit)

```{r p1, exercise=TRUE, eval = FALSE}
1 - pnorm(_, _, _)

# OR

pnorm(_, _, _, lower.tail = FALSE)
```

```{r print-p1-hint}
1 - pnorm(116,100,15)
```


### 2.

Read the ["Lady Tasting Tea" excerpt from ModernDive](https://moderndive.com/previous_versions/v0.2.0/7-sim.html#simulation).

From [Wikipedia](https://en.wikipedia.org/wiki/Lady_tasting_tea):

> "The experiment provided the lady with 8 randomly ordered cups of tea—4 prepared by first adding milk, 4 prepared by first adding the tea. She was to select the 4 cups prepared by one method. This offered the lady the advantage of judging cups by comparison. She was fully informed of the experimental method. The null hypothesis was that the lady had no ability to distinguish the teas. The test statistic was a simple count of the number of successes in selecting the 4 cups (the number of cups of the given type successfully selected)."

Note that she only had to select 4 cups prepared by one method- since she had only 8 cups to choose from, the non-selected 4 cups are assumed to come from the other method. Such an experiment is known to produce a discrete variable with a hypergeometric distribution. This distribution has 3 parameters:

* `m`	the number of white balls in the urn. [cups with milk then tea]	
* `n`	the number of black balls in the urn. [cups with tea then milk]
* `k`	the number of balls drawn from the urn. [number of “draws”]

Where `m + n` is the total number of balls in the urn, or in this case cups of tea. As the story goes, the lady identified all 4 cups correctly (and thus, all 8 cups were correct). What were the chances (exactly!)?


```{r p2, exercise=TRUE, eval = FALSE}
dhyper(_, _, _, _)

# OR

1 - phyper(_, _, _, _)
```

```{r print-p2-hint}
dhyper(4, 4, 4, 4)

# OR

#1 - phyper(3, 4, 4, 4)
```

### 3. 

Find the probability that Muriel guessed 3 or more cups correctly. *Hint: look up `?sum` and `?seq`*

```{r p3, exercise=TRUE, eval = FALSE}
__(dhyper(seq(from = _, to = _), _, _, _))
```

```{r print-p3-hint}
sum(dhyper(seq(from = 3, to = 4), 4, 4, 4))
```

### 4. 

Find the five probabilities associated with Muriel guessing 0, 1, 2, 3, or 4 cups correctly. *Hint: look up `?seq`*

```{r p4, exercise=TRUE, eval = FALSE}
dhyper(seq(from = _, to = _), _, _, _)
```

```{r print-p4-hint}
dhyper(seq(from = 0, to = 4), 4, 4, 4)
```

### 5. 

Now save these numbers into a `tibble`, and make a plot of the probabilities for the hypergeometric distribution on y-axis, across each value of correct cups of tea guessed on the x-axis. Try adding `geom_segment` to represent the distance between the x-axis and the probability.

```{r p5, exercise=TRUE, eval = FALSE}
library(tibble)
muriel <- tibble(
  cups_correct = seq(from = _, to = _),
  prob = dhyper(seq(from = _, to = _), _, _, _)
)
```

```{r print-p5-hint}
muriel <- tibble(
  cups_correct = seq(from = 0, to = 4),
  prob = dhyper(seq(from = 0, to = 4), 4, 4, 4)
)
ggplot(muriel, aes(x = cups_correct, y = prob)) +
  geom_segment(aes(xend = cups_correct, y = 0, yend = prob)) +
  geom_point(colour = "mediumseagreen") 
```



### 6.

Let's say Fisher devises a new experiment with 100 cups of tea total (50% prepared with tea before milk, 50% prepared with milk before tea), and asks Muriel to guess 50 prepared by one method. Here is the distribution for the resulting variable from this experiment:

```{r print-p6-pre}
muriel_half <- tibble(
  cups_correct = seq(from = 0, to = 50),
  prob = dhyper(seq(from = 0, to = 50), 50, 50, 50)
)
ggplot(muriel_half, aes(x = cups_correct, y = prob)) +
  geom_segment(aes(xend = cups_correct, y = 0, yend = prob)) +
  geom_point(colour = "mediumseagreen") 
```

We will reject the null hypothesis that Muriel is "average" if her performance falls outside of the 95% confidence interval for $\mu$. What is the 95% confidence interval?

```{r p6, exercise=TRUE, eval = FALSE}
qhyper(c(_, _), _, _, _) 
```

```{r print-p6-hint}
qhyper(c(.025, .975), 50, 50, 50) 
```


### 7.

In this fictitious experiment with 100 cups of tea, what is the probability that Muriel will get exactly 31 correct?

```{r p7, exercise=TRUE, eval = FALSE}

```

```{r print-p7-hint}
dhyper(31, 50, 50, 50) 
```

### 8.

Inspired by Alan Shepard, the first American to journey into space, a 14-year-old Hillary Rodham from suburban Chicago wrote a letter to NASA in 1961 asking what she needed to do to become an astronaut. 

Let's pretend that, upon receipt of the letter in 1961, NASA decided to conduct a study to test whether girls who are aspiring astronauts in high school have “above average” IQ. Unfortunately, the NASA budget in 1961 was pretty low. So they studied only 25 high school girls, all of whom were aspiring astronauts (AA). Assume that IQ scores in the population are known to be normally distributed with $\mu$ = 100; $\sigma$ = 15.

What is the mean and standard deviation (the “standard error”) of the theoretical null distribution (the sampling distribution of the sample means)?

```{r p8, exercise = TRUE}
null_mu <- 
null_se <- 
```


### 9. 

In our same fictitious aspiring astronauts example, let $\alpha$ = .05 (1-tailed). What value will our sample mean need to “beat” in order for us to conclude that it is higher than the population mean (given random variation present in our sample)? What would you conclude if our sample mean is 104? What if it is 108?


```{r p9, exercise = TRUE}

```

```{r upper_tail_iq}
library(ggplot2)
library(grid)
x <- pretty(90:110, 1000)   
y <- dnorm(x, 100, 3)
df <- data.frame(x = x, y = y)
ggplot(df, aes(x = x, y = y)) + 
  stat_function(fun = dnorm, args = list(mean = 100, sd = 3)) +
  geom_ribbon(data = subset(df, x > qnorm(.95, 100, 3)), 
              aes(ymin = 0, ymax = y), 
              alpha = 0.25) +
  geom_segment(aes(x = 100, 
                   y = 0, 
                   xend = 100, 
                   yend = y), 
               linetype = "dotted") +
  geom_segment(aes(x = qnorm(.95, 100, 3), 
                   y = 0, 
                   xend = qnorm(.95, 100, 3), 
                   yend = dnorm(qnorm(.95, 100, 3), 100, 3)),
               linetype = "solid") +
  geom_segment(aes(x = min(x), 
                   y = 0, 
                   xend = max(x), 
                   yend = 0),
               linetype = "solid") +
  theme(axis.title = element_blank(), 
        panel.grid = element_blank(), 
        panel.background=element_blank(),
        axis.ticks = element_blank(),
        axis.text.y = element_blank()) +
  annotate("text", x = qnorm(.95, 100, 3) + 3, 
           y = 0.02, 
           label = "p = 0.05", 
           size = 4)  +
  geom_segment(aes(x = qnorm(.95, 100, 3), 
                   y = dnorm(qnorm(.90, 100, 3), 100, 3), 
                   xend = qnorm(.95, 100, 3), 
                   yend = dnorm(qnorm(.94, 100, 3), 100, 3)), 
               arrow = arrow(length = unit(0.2, "cm"))) +
  annotate("text", x = qnorm(.95, 100, 3), 
           y = dnorm(qnorm(.89, 100, 3), 100, 3), 
           label = "q = ?", 
           size = 4)
```

### 10.

In our same fictitious aspiring astronauts example, let $\alpha$ = .05 (2-tailed). What value(s) will our sample mean need to “beat” in order for us to conclude that it is different than the population mean (given random variation present in our sample)? What would you conclude if our sample mean is 105? 

```{r p10, exercise = TRUE}

```

```{r two_tail_iq}
library(ggplot2)
library(grid)
x <- pretty(90:110, 1000)   
y <- dnorm(x, 100, 3)
df <- data.frame(x = x, y = y)
two_tail_z <- ggplot(df, aes(x = x, y = y)) + 
  stat_function(fun = dnorm, args = list(mean = 100, sd = 3)) +
  geom_ribbon(data = subset(df, x > qnorm(.975, 100, 3)), 
              aes(ymin = 0, ymax = y), 
              alpha = 0.25) +
  geom_ribbon(data = subset(df, x < qnorm(.025, 100, 3)), 
              aes(ymin = 0, ymax = y), 
              alpha = 0.25) +
  geom_segment(aes(x = 100, 
                   y = 0, 
                   xend = 100, 
                   yend = y), 
               linetype = "dotted") +
  geom_segment(aes(x = qnorm(.975, 100, 3), 
                   y = 0, 
                   xend = qnorm(.975, 100, 3), 
                   yend = dnorm(qnorm(.975, 100, 3), 100, 3)),
               linetype = "solid") +
  geom_segment(aes(x = qnorm(.025, 100, 3), 
                   y = 0, 
                   xend = qnorm(.025, 100, 3), 
                   yend = dnorm(qnorm(.025, 100, 3), 100, 3)),
               linetype = "solid") +
    geom_segment(aes(x = min(x), 
                   y = 0, 
                   xend = max(x), 
                   yend = 0),
               linetype = "solid") +
  theme(axis.title = element_blank(), 
        panel.grid = element_blank(), 
        panel.background=element_blank(),
        axis.ticks = element_blank(),
        axis.text.y = element_blank()) +
  annotate("text", x = qnorm(.95, 100, 3) + 3, 
           y = 0.02, 
           label = "p = 0.025", 
           size = 4)  +
  geom_segment(aes(x = qnorm(.975, 100, 3), 
                   y = dnorm(qnorm(.95, 100, 3), 100, 3), 
                   xend = qnorm(.975, 100, 3), 
                   yend = dnorm(qnorm(.97, 100, 3), 100, 3)), 
               arrow = arrow(length = unit(0.2, "cm"))) +
  annotate("text", x = qnorm(.98, 100, 3), 
           y = dnorm(qnorm(.94, 100, 3), 100, 3), 
           label = "q = ?", 
           size = 4) +
  annotate("text", x = qnorm(.05, 100, 3) - 3, 
           y = 0.02, 
           label = "p = 0.025", 
           size = 4)  +
  geom_segment(aes(x = qnorm(.025, 100, 3), 
                   y = dnorm(qnorm(.05, 100, 3), 100, 3), 
                   xend = qnorm(.025, 100, 3), 
                   yend = dnorm(qnorm(.03, 100, 3), 100, 3)), 
               arrow = arrow(length = unit(0.2, "cm"))) +
  annotate("text", x = qnorm(.02, 100, 3), 
           y = dnorm(qnorm(.06, 100, 3), 100, 3), 
           label = "q = ?", 
           size = 4)
two_tail_z
two_tail_z + geom_vline(aes(xintercept = 105), colour = "red", lty = "dashed")
```


## Finding critical values

> __Critical Value(s)__: the value of a given test statistic that corresponds to a rejection point- the point at which you determine to reject the null hypothesis. The critical value defines the boundary of the rejection region for $H_0$. This value depends on the significance level, $\alpha$, and whether the test is one-sided or two-sided.

To find critical values, you'll want to use `q(*)`. This function takes a probability (between 0 and 1) as input, and returns the quantile for the `*` distribution.

### 1.

Suppose you want to test the null hypothesis that $\mu$=100 with a sample size of *n*=25 and an $\alpha$=.05. What will the critical value(s) for the *t* statistic be?

```{r crit1, exercise=TRUE, eval = FALSE}
qt(c(_, _), _) 
```

```{r print-crit1-hint}
qt(c(.025, .975), 24)
```


### 2.

Suppose you want to test the null hypothesis that $\mu\leq{100}$ with a sample size *n*=60 and an $\alpha$=.01. What will the critical value(s) for the *t* statistic be?

```{r crit2, exercise=TRUE, eval = FALSE}
qt(_, _) # upper only
```

```{r print-crit2-hint}
qt(0.99, 59)
```


### 3.

You conduct a one-sample *t* test with *N*=101 and report a *p*-value of .0245. What is the *t*-statistic value? 

```{r crit3, exercise=TRUE, eval = FALSE}
qt(_, _)
```

```{r print-crit3-hint}
qt(.0245, 100)
```

### 4.

What are the critical values of *t* for *N*=8; $\alpha$=.05 using a directional hypothesis in the upper tail (assume testing of means)?

```{r crit4, exercise=TRUE, eval = FALSE}
qt(_, _)
```

```{r print-crit4-hint}
qt(.95, 7)
```

### 5.

What are the critical values of *t* for *N*=15; $\alpha$=.01 using a directional hypothesis in the upper tail (assume testing of means)?

```{r crit5, exercise=TRUE, eval = FALSE}
qt(_, _)
```

```{r print-crit5-hint}
qt(.99, 14)
```

### 6.

What are the critical values of *t* for *N*=51; $\alpha$=.025 using a directional hypothesis in the upper tail (assume testing of means)?

```{r crit6, exercise=TRUE, eval = FALSE}
qt(_, _)
```

```{r print-crit6-hint}
qt(.975, 50)
```

### 7.

What are the critical values of *t* for *N*=12; $\alpha$=.05 using a directional hypothesis in the upper tail (assume testing of means)?

```{r crit7, exercise=TRUE, eval = FALSE}
qt(c(_, _), _)
```

```{r print-crit7-hint}
qt(c(.025, .975), 11)
```

### 8.

What are the critical values of *t* for *N*=20; $\alpha$=.01 using a directional hypothesis in the upper tail (assume testing of means)?

```{r crit8, exercise=TRUE, eval = FALSE}
qt(c(_, _), _)
```

```{r print-crit8-hint}
qt(c(.005, .995), 19)
```

### 9.

What are the critical values of *t* for *N*=2; $\alpha$=.05 using a directional hypothesis in the upper tail (assume testing of means)?

```{r crit9, exercise=TRUE, eval = FALSE}
qt(c(_, _), _)
```

```{r print-crit9-hint}
qt(c(.025, .975), 1)
```



## Finding confidence intervals

### 1.

Given the following sample statistics ($\bar{x}$=13.0, sd=1.6, *N*=21), what is the 95% confidence interval for $\mu$?

```{r ci1, exercise=TRUE, eval = FALSE}
xbar <- 13
sd <- 1.6
N <- 21
```

```{r print-ci1-hint}
xbar <- 13
sd <- 1.6
N <- 21
se <- sd/sqrt(N)
crit_t <- qt(.975, N-1)
c(xbar - crit_t*se, xbar + crit_t*se)
```

### 2. 

Let's return to our aspiring astronauts. What is the 95% confidence interval for $\mu$ (assume 2-tailed)? Here are the statistics you need:

```{r ci2-pre}
# sample statistics
xbar <- 105
n_samp <- 25

# population statistics
mu0 <- 100
sigma <- 15
```

```{r ci2, exercise = TRUE}

```

```{r print-ci2-hint}
# margin of error
me <- qnorm(.975) * (sigma/sqrt(n_samp)) # .975 --> .025 at EACH tail

# 95% confidence intervals
lowerz <- xbar - me
upperz <- xbar + me
c(lowerz, upperz)

# even quicker!
#(confintz <- qnorm(c(.025, .975), xbar, sigma/sqrt(n_samp)))
```

### 3. 

Let's return to our aspiring astronauts, and assume that we actually don't know $\sigma$, the standard deviation of IQ scores in the population, but we choose to estimate it with our sample standard deviation. What is the 95% confidence interval for $\mu$ (assume 2-tailed)? Here are the statistics you need:

```{r ci3-pre}
# sample statistics
xbar <- 105
n_samp <- 25
sd <- 13
```

```{r ci3, exercise = TRUE}

```

```{r print-ci3-hint}
# sample statistics
xbar <- 105
sd <- 13
n_samp <- 25

# margin of error
me <- qt(.975, n_samp - 1) * (sd/sqrt(n_samp)) # .975 --> .025 at EACH tail

# 95% confidence intervals
lowert <- xbar - me
uppert <- xbar + me
c(lowert, uppert)
```

